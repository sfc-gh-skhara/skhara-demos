{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7d72538",
   "metadata": {},
   "source": [
    "## Steps:\n",
    "- Import libraries\n",
    "- Setup Snowflake objects\n",
    "- Load data to Snowflake (you may skip this step if you already have data in Snowflake)\n",
    "- Write code to run Implicit library locally on your machine\n",
    "- Package code to and make it clean\n",
    "- Create a Task and schedule it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa08deeb",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cb04fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.session import Session ./\n",
    "import snowflake.snowpark.types as T\n",
    "import snowflake.snowpark.functions as F\n",
    "from snowflake.snowpark.functions import when, col\n",
    "from snowflake.snowpark.window import Window\n",
    "from snowflake.snowpark.functions import dense_rank\n",
    "\n",
    "from snowflake.core import Root\n",
    "from snowflake.core.task import StoredProcedureCall, Task\n",
    "from snowflake.core.task.dagv1 import DAGOperation, DAG, DAGTask\n",
    "\n",
    "import json\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import implicit \n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cc080c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_parameters = json.load(open('/Users/skhara/Documents/GitHub/creds.json'))\n",
    "session = Session.builder.configs(connection_parameters).create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "605182bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='ML_MODELS already exists, statement succeeded.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql('CREATE DATABASE IF NOT EXISTS RECOMMENDER_SYSTEMS').collect()\n",
    "session.sql('CREATE SCHEMA IF NOT EXISTS RECOMMENDER_SYSTEMS.COLLABORATIVE_FILTERING_ALS').collect()\n",
    "\n",
    "session.sql('USE DATABASE RECOMMENDER_SYSTEMS').collect()\n",
    "session.sql('USE SCHEMA COLLABORATIVE_FILTERING_ALS').collect()\n",
    "session.sql('CREATE STAGE IF NOT EXISTS ML_MODELS;').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c33e1c",
   "metadata": {},
   "source": [
    "# Load Data to Snowflake\n",
    "This is done in case your data is not already in a Snowflake table. If it is then you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "acdb5f2e-7dd3-4a2e-af77-14bbcde7202f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>visitorid</th>\n",
       "      <th>event</th>\n",
       "      <th>itemid</th>\n",
       "      <th>transactionid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1433221332117</td>\n",
       "      <td>257597</td>\n",
       "      <td>view</td>\n",
       "      <td>355908</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1433224214164</td>\n",
       "      <td>992329</td>\n",
       "      <td>view</td>\n",
       "      <td>248676</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1433221999827</td>\n",
       "      <td>111016</td>\n",
       "      <td>view</td>\n",
       "      <td>318965</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1433221955914</td>\n",
       "      <td>483717</td>\n",
       "      <td>view</td>\n",
       "      <td>253185</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1433221337106</td>\n",
       "      <td>951259</td>\n",
       "      <td>view</td>\n",
       "      <td>367447</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       timestamp  visitorid event  itemid  transactionid\n",
       "0  1433221332117     257597  view  355908            NaN\n",
       "1  1433224214164     992329  view  248676            NaN\n",
       "2  1433221999827     111016  view  318965            NaN\n",
       "3  1433221955914     483717  view  253185            NaN\n",
       "4  1433221337106     951259  view  367447            NaN"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading from local CSV-files\n",
    "events_data = pd.read_csv('data/events.csv')\n",
    "events_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d013e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.write_pandas(events_data, table_name='EVENTS_DATA', auto_create_table=True, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5e80fca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning column names to make it easier for future referencing\n",
    "snow_table = session.table('EVENTS_DATA')\n",
    "\n",
    "import re\n",
    "\n",
    "cols = snow_table.columns\n",
    "for old_col in cols:\n",
    "    new_col = re.sub(r'[^a-zA-Z0-9_]', '', old_col)\n",
    "    new_col = new_col.upper()\n",
    "    snow_table = snow_table.rename(col(old_col), new_col)\n",
    "snow_table.write.mode(\"overwrite\").save_as_table(\"EVENTS_DATA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5a6ae0",
   "metadata": {},
   "source": [
    "# Step 1: Testing Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "8c4bbf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Snowpark Dataframes for Push-down compute.\n",
    "datapath= 'RECOMMENDER_SYSTEMS.COLLABORATIVE_FILTERING_ALS.EVENTS_DATA'\n",
    "snf_df = session.table(datapath)\n",
    "snf_df = snf_df.with_column(\"TS_DATE\", F.to_date(F.to_timestamp(F.col('TIMESTAMP')/F.lit(1000))))\n",
    "snf_df = snf_df.withColumn(\"EVENT_ID\", when(col(\"EVENT\") == \"transaction\", 3)\n",
    "                                        .when(col(\"EVENT\") == \"addtocart\", 2)\n",
    "                                        .when(col(\"EVENT\") == \"view\", 1))\n",
    "\n",
    "window_spec = Window.partition_by().order_by(\"VISITORID\")\n",
    "snf_df = snf_df.with_column(\"VISITOR_ID\", dense_rank().over(window_spec))\n",
    "\n",
    "window_spec_item = Window.partition_by().order_by(\"ITEMID\")\n",
    "snf_df = snf_df.with_column(\"ITEM_ID\", dense_rank().over(window_spec_item))\n",
    "\n",
    "snf_df.write.mode(\"overwrite\").save_as_table(\"EVENTS_DATA_CLEANED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "be525967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>VISITORID</th>\n",
       "      <th>EVENT</th>\n",
       "      <th>ITEMID</th>\n",
       "      <th>TRANSACTIONID</th>\n",
       "      <th>TS_DATE</th>\n",
       "      <th>EVENT_ID</th>\n",
       "      <th>VISITOR_ID</th>\n",
       "      <th>ITEM_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1434389885755</td>\n",
       "      <td>290541</td>\n",
       "      <td>view</td>\n",
       "      <td>427952</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-06-15</td>\n",
       "      <td>1</td>\n",
       "      <td>290542</td>\n",
       "      <td>215538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1434409631844</td>\n",
       "      <td>300845</td>\n",
       "      <td>view</td>\n",
       "      <td>427935</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-06-15</td>\n",
       "      <td>1</td>\n",
       "      <td>300846</td>\n",
       "      <td>215526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1434402529968</td>\n",
       "      <td>391960</td>\n",
       "      <td>view</td>\n",
       "      <td>236503</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-06-15</td>\n",
       "      <td>1</td>\n",
       "      <td>391961</td>\n",
       "      <td>119073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1434405086107</td>\n",
       "      <td>536804</td>\n",
       "      <td>view</td>\n",
       "      <td>355266</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-06-15</td>\n",
       "      <td>1</td>\n",
       "      <td>536805</td>\n",
       "      <td>179017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1434343357382</td>\n",
       "      <td>1108722</td>\n",
       "      <td>view</td>\n",
       "      <td>174284</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-06-15</td>\n",
       "      <td>1</td>\n",
       "      <td>1108723</td>\n",
       "      <td>87811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       TIMESTAMP  VISITORID EVENT  ITEMID  TRANSACTIONID     TS_DATE  \\\n",
       "0  1434389885755     290541  view  427952            NaN  2015-06-15   \n",
       "1  1434409631844     300845  view  427935            NaN  2015-06-15   \n",
       "2  1434402529968     391960  view  236503            NaN  2015-06-15   \n",
       "3  1434405086107     536804  view  355266            NaN  2015-06-15   \n",
       "4  1434343357382    1108722  view  174284            NaN  2015-06-15   \n",
       "\n",
       "   EVENT_ID  VISITOR_ID  ITEM_ID  \n",
       "0         1      290542   215538  \n",
       "1         1      300846   215526  \n",
       "2         1      391961   119073  \n",
       "3         1      536805   179017  \n",
       "4         1     1108723    87811  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snf_cleaned = session.table('EVENTS_DATA_CLEANED')\n",
    "snf_cleaned.limit(5).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "1c2cb11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = snf_cleaned.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "75499af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2015, 9, 18)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(data['TS_DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "37be9cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "645cac6a5aaa4064bcb3977251b6b8f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sparse_item_user = sparse.csr_matrix((data['EVENT_ID'].astype(float), (data['ITEM_ID'], data['VISITOR_ID'])))\n",
    "sparse_user_item = sparse.csr_matrix((data['EVENT_ID'].astype(float), (data['VISITOR_ID'], data['ITEM_ID'])))\n",
    "\n",
    "alpha_val = 40\n",
    "data_conf = (sparse_user_item * alpha_val).astype('double')\n",
    "\n",
    "#Building the model\n",
    "model = implicit.als.AlternatingLeastSquares(factors=20, regularization=0.1, iterations=20)\n",
    "model.fit(data_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0b8f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Recommendations\n",
    "user_id = 2\n",
    "reco = model.recommend(user_id, sparse_user_item[user_id], N=5)\n",
    "print(reco)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79484e7",
   "metadata": {},
   "source": [
    "# Option 1: SPROC Based Orchestration in Snowflake\n",
    "\n",
    "Here we take all the pieces of code written above for local testing and package in a modularized format. We are choosing to\n",
    "schedule the preprocess pipeline as a predecessor to model train and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "87a92a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.core import Root\n",
    "from snowflake.core.task import StoredProcedureCall, Task\n",
    "from snowflake.core.task.dagv1 import DAGOperation, DAG, DAGTask\n",
    "api_root = Root(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "fd0386b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Preprocess Data\n",
    "def preprocess_data(session:Session) -> str:\n",
    "    datapath= 'RECOMMENDER_SYSTEMS.COLLABORATIVE_FILTERING_ALS.EVENTS_DATA'\n",
    "    snf_df = session.table(datapath)\n",
    "    snf_df = snf_df.with_column(\"TS_DATE\", F.to_date(F.to_timestamp(F.col('TIMESTAMP')/F.lit(1000))))\n",
    "    snf_df = snf_df.sort(F.col('\"TS_DATE\"').asc())\n",
    "\n",
    "    snf_df = snf_df.withColumn(\"EVENT_ID\", when(col(\"EVENT\") == \"transaction\", 3)\n",
    "                                            .when(col(\"EVENT\") == \"addtocart\", 2)\n",
    "                                            .when(col(\"EVENT\") == \"view\", 1))\n",
    "\n",
    "    window_spec = Window.partition_by().order_by(\"VISITORID\")\n",
    "    snf_df = snf_df.with_column(\"VISITOR_ID\", dense_rank().over(window_spec))\n",
    "\n",
    "    window_spec_item = Window.partition_by().order_by(\"ITEMID\")\n",
    "    snf_df = snf_df.with_column(\"ITEM_ID\", dense_rank().over(window_spec_item))\n",
    "\n",
    "    snf_df.write.mode(\"overwrite\").save_as_table(\"EVENTS_DATA_CLEANED\")\n",
    "\n",
    "    return 'DATA PROCESSING SUCCESS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "b0e89a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The version of package 'joblib' in the local environment is 1.3.2, which does not fit the criteria for the requirement 'joblib'. Your UDF might not work when the package version is different between the server and your local environment.\n"
     ]
    }
   ],
   "source": [
    "# Registering the function as a Stored Procedure\n",
    "sproc_model_train = session.sproc.register(func=preprocess_data,\n",
    "                                           name='ALS_DATA_PREPROCESS',\n",
    "                                           is_permanent=True,\n",
    "                                           replace=True,\n",
    "                                           stage_location='@ML_MODELS',\n",
    "                                           packages=[\"snowflake-snowpark-python\",\"snowflake-ml-python\", \"joblib\",\n",
    "                                                     \"regex\", \"scipy\", \"implicit==0.6.2\", \"numpy==1.23.5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "354d46b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 1 - SMALL\n",
    "session.sql('USE WAREHOUSE DEMO_STANDARD;').collect()\n",
    "session.sql('ALTER WAREHOUSE DEMO_STANDARD SET WAREHOUSE_SIZE = \"MEDIUM\"').collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "abede6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Model Train + Inference\n",
    "def train_model(session:Session, sparse_user_item):\n",
    "    import implicit\n",
    "    import scipy.sparse as sparse\n",
    "\n",
    "    alpha_val = 40\n",
    "    data_conf = (sparse_user_item * alpha_val).astype('double')\n",
    "\n",
    "    model = implicit.als.AlternatingLeastSquares(factors=20, regularization=0.1, iterations=20)\n",
    "    model.fit(data_conf)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Run the Process\n",
    "def get_predictions(session:Session) -> str:\n",
    "    import pandas as pd\n",
    "    import implicit\n",
    "    import scipy.sparse as sparse\n",
    "    from datetime import datetime\n",
    "    import snowflake.snowpark.functions as F\n",
    "\n",
    "    data = session.table('RECOMMENDER_SYSTEMS.COLLABORATIVE_FILTERING_ALS.EVENTS_DATA_CLEANED').to_pandas()\n",
    "\n",
    "    # Sparse matrix are more performant when the range of numbers isnt too large\n",
    "    # sparse_item_user = sparse.csr_matrix((data['event'].astype(float), (data['item_id'], data['visitor_id'])))\n",
    "    sparse_user_item = sparse.csr_matrix((data['EVENT_ID'].astype(float), (data['VISITOR_ID'], data['ITEM_ID'])))\n",
    "    model = train_model(session, sparse_user_item)\n",
    "    recommended = model.recommend(data['VISITOR_ID'].to_list(), sparse_user_item[data['VISITOR_ID'].to_list()], N=5)\n",
    "    rec_df = pd.DataFrame(reco[0], columns=['rec1', 'rec2', 'rec3', 'rec4', 'rec5'])\n",
    "\n",
    "    # Save Data in Snowflake\n",
    "    session.write_pandas(rec_df, table_name='ITEM_RECOMMENDATIONS', auto_create_table=True, overwrite=True)\n",
    "    return 'Success'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "717bd4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = api_root.databases['RECOMMENDER_SYSTEMS'].schemas['COLLABORATIVE_FILTERING_ALS']\n",
    "tasks = schema.tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c9891233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Task 1: Preprocess Task\n",
    "task1_entity = Task(\n",
    "    \"PREPROCESS_DATA\",\n",
    "    definition = StoredProcedureCall(preprocess_data,\n",
    "                                   stage_location=\"@ML_MODELS\",\n",
    "                                   packages=[\"snowflake-snowpark-python\",\"snowflake-ml-python\", \"regex\"]),\n",
    "    warehouse = connection_parameters['warehouse'],\n",
    "    schedule = timedelta(days=1))\n",
    "\n",
    "task1 = tasks.create(task1_entity, mode=\"orReplace\")\n",
    "\n",
    "# Create Task 2: for model training and inference\n",
    "task2_entity = Task(\n",
    "    \"RECO_ENGINE\",\n",
    "    definition = StoredProcedureCall(get_predictions, stage_location=\"@ML_MODELS\", \n",
    "                                     packages=[\"snowflake-snowpark-python\",\"snowflake-ml-python\",\n",
    "                                               \"regex\", \"scipy\", \"implicit==0.6.2\", \"numpy==1.23.5\"]),\n",
    "    warehouse = connection_parameters['warehouse']\n",
    "    )\n",
    "\n",
    "task2_entity.predecessors = [\"RECOMMENDER_SYSTEMS.COLLABORATIVE_FILTERING_ALS.PREPROCESS_DATA\"]\n",
    "task2 = tasks.create(task2_entity, mode=\"orReplace\")\n",
    "\n",
    "task2.resume()\n",
    "task1.resume()\n",
    "task1.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5dc1d9",
   "metadata": {},
   "source": [
    "# Option 2: Distributed Modeling using UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0b79b0",
   "metadata": {},
   "source": [
    "### Model Training using SPROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16cb50cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training SPROC\n",
    "def model_train(session:Session, table_name: str) -> str:\n",
    "    import pandas as pd\n",
    "    import joblib, implicit\n",
    "    import scipy.sparse as sparse\n",
    "    from scipy.sparse import save_npz\n",
    "\n",
    "    data = session.table(table_name).to_pandas()\n",
    "\n",
    "    # Sparse matrix are more performant when the range of numbers isnt too large\n",
    "    sparse_user_item = sparse.csr_matrix((data['EVENT_ID'].astype(float), (data['VISITOR_ID'], data['ITEM_ID'])))\n",
    "\n",
    "    # Model Training\n",
    "    alpha_val = 40\n",
    "    data_conf = (sparse_user_item * alpha_val).astype('double')\n",
    "    model = implicit.als.AlternatingLeastSquares(factors=20, regularization=0.1, iterations=20)\n",
    "    model.fit(data_conf)\n",
    "\n",
    "    # Serialize sparse_user_item\n",
    "    save_npz('/tmp/sparse_user_item.npz', sparse_user_item)\n",
    "    session.file.put('/tmp/sparse_user_item.npz', '@ML_MODELS/TRAIN_OUTPUT', auto_compress=False, overwrite=True)\n",
    "\n",
    "    # Save model file\n",
    "    FILE_LOCATION = '/tmp/als_model.joblib'\n",
    "    joblib.dump(model, FILE_LOCATION)\n",
    "    session.file.put(FILE_LOCATION, '@ML_MODELS/TRAIN_OUTPUT', auto_compress=False, overwrite=True)\n",
    "    \n",
    "    return 'Success'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e593246",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The version of package 'joblib' in the local environment is 1.3.2, which does not fit the criteria for the requirement 'joblib'. Your UDF might not work when the package version is different between the server and your local environment.\n"
     ]
    }
   ],
   "source": [
    "# Registering the function as a Stored Procedure\n",
    "sproc_model_train = session.sproc.register(func=model_train,\n",
    "                                           name='ALS_MODEL_TRAIN',\n",
    "                                           is_permanent=True,\n",
    "                                           replace=True,\n",
    "                                           stage_location='@ML_MODELS',\n",
    "                                           packages=[\"snowflake-snowpark-python\",\"snowflake-ml-python\", \"joblib\",\n",
    "                                                     \"regex\", \"scipy\", \"implicit==0.6.2\", \"numpy==1.23.5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "809d0117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 3 - SOWH MEDIUM\n",
    "session.sql('USE WAREHOUSE SSK_RESEARCH;').collect()\n",
    "session.sql('ALTER WAREHOUSE DEMO_STANDARD SET WAREHOUSE_SIZE = \"MEDIUM\"').collect()\n",
    "\n",
    "table_name = 'RECOMMENDER_SYSTEMS.COLLABORATIVE_FILTERING_ALS.EVENTS_DATA_CLEANED'\n",
    "_ = session.call(\"ALS_MODEL_TRAIN\", table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b0be3f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Success'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bf2210",
   "metadata": {},
   "source": [
    "### UDF for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "6826392c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['VISITORID','VISITOR_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "d4080e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple scoring function\n",
    "from cachetools import cached\n",
    "\n",
    "@cached(cache={})\n",
    "def load_from_stage(import_dir) -> object:\n",
    "    import joblib\n",
    "    from scipy.sparse import load_npz\n",
    "    model = joblib.load(import_dir + 'als_model.joblib') # Load Model\n",
    "    sparse_user_item = load_npz(import_dir + 'sparse_user_item.npz') # Load the Sparse input file\n",
    "    return model, sparse_user_item\n",
    "\n",
    "def udf_als_score(df: T.PandasDataFrame[int, int]) -> T.PandasSeries[dict]:\n",
    "    import sys, implicit\n",
    "    # file-dependencies of UDFs are available in snowflake_import_directory\n",
    "    IMPORT_DIRECTORY_NAME = \"snowflake_import_directory\"\n",
    "    import_dir = sys._xoptions[IMPORT_DIRECTORY_NAME]\n",
    "\n",
    "    model, sparse_user_item = load_from_stage(import_dir)\n",
    "    df.columns = feature_cols\n",
    "    recommended = model.recommend(df['VISITOR_ID'].to_list(), sparse_user_item[df['VISITOR_ID'].to_list()], N=5)\n",
    "\n",
    "    # Processing for output\n",
    "    recommended_series = pd.Series([{\"recommendations\": row.tolist()} for row in recommended[0]])\n",
    "    return recommended_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "7fca743e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The version of package 'joblib' in the local environment is 1.3.2, which does not fit the criteria for the requirement 'joblib'. Your UDF might not work when the package version is different between the server and your local environment.\n"
     ]
    }
   ],
   "source": [
    "# Register UDF\n",
    "udf_als = session.udf.register(func=udf_als_score, \n",
    "                               name=\"ALS_COLAB_FILTERING\", \n",
    "                               stage_location='@ML_MODELS',\n",
    "                               replace=True,\n",
    "                               is_permanent=True, \n",
    "                               imports=['@ML_MODELS/TRAIN_OUTPUT/als_model.joblib',\n",
    "                                        '@ML_MODELS/TRAIN_OUTPUT/sparse_user_item.npz'],\n",
    "                               packages=[\"snowflake-snowpark-python\",\"snowflake-ml-python\", \"joblib\",\n",
    "                                         \"regex\", \"scipy\", \"implicit==0.6.2\", \"numpy==1.23.5\"],\n",
    "                               session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "7da18fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 5 - SOWH MEDIUM\n",
    "session.sql('USE WAREHOUSE SSK_RESEARCH;').collect()\n",
    "session.sql('ALTER WAREHOUSE DEMO_STANDARD SET WAREHOUSE_SIZE = \"MEDIUM\"').collect()\n",
    "\n",
    "feature_cols = ['VISITORID','VISITOR_ID']\n",
    "snowdf_test = session.table('RECOMMENDER_SYSTEMS.COLLABORATIVE_FILTERING_ALS.EVENTS_DATA_CLEANED').select(feature_cols)\n",
    "snowdf_test = snowdf_test.drop_duplicates('VISITORID')\n",
    "test_sdf_w_preds = snowdf_test.with_column('PREDICTED', F.call_udf(\"ALS_COLAB_FILTERING\", [F.col(c) for c in feature_cols]))\\\n",
    "                                .with_column(\"RECOMMENDATIONS\", F.col(\"PREDICTED\")['recommendations'])\n",
    "test_sdf_w_preds.write.mode(\"overwrite\").save_as_table(\"RECOMMENDATIONS_OUTPUT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9069dbc",
   "metadata": {},
   "source": [
    "### Orchestration Using Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3d994e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Train and Predict\n",
    "def train_and_predict(session:Session) -> str:\n",
    "    from snowflake.snowpark.functions import udf\n",
    "    import snowflake.snowpark.functions as F\n",
    "\n",
    "    # Call SPROC\n",
    "    table_name = 'RECOMMENDER_SYSTEMS.COLLABORATIVE_FILTERING_ALS.EVENTS_DATA_CLEANED'\n",
    "    _ = session.call(\"ALS_MODEL_TRAIN\", table_name)\n",
    "\n",
    "    # Prediction using UDF\n",
    "    feature_cols = ['VISITORID','VISITOR_ID']\n",
    "    snowdf_test = session.table('RECOMMENDER_SYSTEMS.COLLABORATIVE_FILTERING_ALS.EVENTS_DATA_CLEANED').select(feature_cols)\n",
    "    snowdf_test = snowdf_test.drop_duplicates('VISITORID')\n",
    "\n",
    "    test_sdf_w_preds = snowdf_test.with_column('PREDICTED', F.call_udf(\"ALS_COLAB_FILTERING\", [F.col(c) for c in feature_cols]))\\\n",
    "                                    .with_column(\"RECOMMENDATIONS\", F.col(\"PREDICTED\")['recommendations'])\n",
    "\n",
    "    test_sdf_w_preds.write.mode(\"overwrite\").save_as_table(\"RECOMMENDATIONS_OUTPUT\")\n",
    "\n",
    "    return 'Recommendation Model Success'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "47975b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Task Object\n",
    "api_root = Root(session)\n",
    "schema = api_root.databases['RECOMMENDER_SYSTEMS'].schemas['COLLABORATIVE_FILTERING_ALS']\n",
    "tasks = schema.tasks\n",
    "\n",
    "# Create the Task\n",
    "task1_entity = Task(\n",
    "    \"TRAIN_AND_PREDICT\",\n",
    "    definition = StoredProcedureCall(train_and_predict,\n",
    "                                     stage_location=\"@ML_MODELS\",\n",
    "                                     packages=[\"snowflake-snowpark-python\",\"snowflake-ml-python\", \"regex\"]),\n",
    "    warehouse = connection_parameters['warehouse'],\n",
    "    schedule = timedelta(days=1))\n",
    "\n",
    "task1 = tasks.create(task1_entity, mode=\"orReplace\")\n",
    "task1.resume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "991d8e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "task1.execute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
