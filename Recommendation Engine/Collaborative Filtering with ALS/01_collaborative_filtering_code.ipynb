{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7d72538",
   "metadata": {},
   "source": [
    "## Steps:\n",
    "- Import libraries\n",
    "- Setup Snowflake objects\n",
    "- Load data to Snowflake (you may skip this step if you already have data in Snowflake)\n",
    "- Write code to run Implicit library locally on your machine\n",
    "- Package code to and make it clean\n",
    "- Create a Task and schedule it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa08deeb",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cb04fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.session import Session\n",
    "import snowflake.snowpark.types as T\n",
    "import snowflake.snowpark.functions as F\n",
    "from snowflake.snowpark.functions import col\n",
    "\n",
    "from snowflake.core import Root\n",
    "from snowflake.core.task import StoredProcedureCall, Task\n",
    "from snowflake.core.task.dagv1 import DAGOperation, DAG, DAGTask\n",
    "\n",
    "import json\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import implicit \n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cc080c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_parameters = json.load(open('/Users/skhara/Documents/GitHub/creds.json'))\n",
    "session = Session.builder.configs(connection_parameters).create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "605182bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='ML_MODELS already exists, statement succeeded.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql('CREATE DATABASE IF NOT EXISTS RECOMMENDER_SYSTEMS').collect()\n",
    "session.sql('CREATE SCHEMA IF NOT EXISTS RECOMMENDER_SYSTEMS.COLLABORATIVE_FILTERING_ALS').collect()\n",
    "\n",
    "session.sql('USE DATABASE RECOMMENDER_SYSTEMS').collect()\n",
    "session.sql('USE SCHEMA COLLABORATIVE_FILTERING_ALS').collect()\n",
    "session.sql('CREATE STAGE IF NOT EXISTS ML_MODELS;').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9306364",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql('ALTER WAREHOUSE SSK_RESEARCH SET max_concurrency_level = 1;').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c33e1c",
   "metadata": {},
   "source": [
    "# Load Data to Snowflake\n",
    "This is done in case your data is not already in a Snowflake table. If it is then you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdb5f2e-7dd3-4a2e-af77-14bbcde7202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading from local CSV-files\n",
    "events_data = pd.read_csv('data/events.csv')\n",
    "events_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d013e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.write_pandas(events_data, table_name='EVENTS_DATA', auto_create_table=True, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5a6ae0",
   "metadata": {},
   "source": [
    "# Step 1: Testing Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4bbf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath= 'RECOMMENDER_SYSTEMS.COLLABORATIVE_FILTERING_ALS.EVENTS_DATA'\n",
    "snf_df = session.table(datapath)\n",
    "snf_df = snf_df.with_column(\"TS_DATE\", F.to_date(F.to_timestamp(F.col('\"timestamp\"')/F.lit(1000))))\n",
    "snf_df = snf_df.sort(F.col('\"TS_DATE\"').asc())\n",
    "\n",
    "data = snf_df.to_pandas()\n",
    "data['visitorid'] = data['visitorid'].astype(\"category\")\n",
    "data['visitor_id'] = data['visitorid'].cat.codes\n",
    "data['visitorid'] = data['visitorid'].astype(\"int\")\n",
    "\n",
    "data['itemid'] = data['itemid'].astype(\"category\")\n",
    "data['item_id'] = data['itemid'].cat.codes\n",
    "data['itemid'] = data['itemid'].astype(\"int\")\n",
    "\n",
    "data['event']= data['event'].astype('category')\n",
    "data['event']= data['event'].cat.codes\n",
    "data['event']= data['event'].astype('int')\n",
    "\n",
    "rename_dict = {old_col: re.sub(r'[^a-zA-Z0-9_]', '', old_col).upper() for old_col in data.columns}\n",
    "data.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "session.write_pandas(data, table_name='EVENTS_DATA_CLEANED', auto_create_table=True, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8410a35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.write_pandas(data, table_name='EVENTS_DATA_CLEANED', auto_create_table=True, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be525967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>VISITORID</th>\n",
       "      <th>EVENT</th>\n",
       "      <th>ITEMID</th>\n",
       "      <th>TRANSACTIONID</th>\n",
       "      <th>TS_DATE</th>\n",
       "      <th>VISITOR_ID</th>\n",
       "      <th>ITEM_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1440795810142</td>\n",
       "      <td>922839</td>\n",
       "      <td>2</td>\n",
       "      <td>387334</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-28</td>\n",
       "      <td>922839</td>\n",
       "      <td>195153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1440796866269</td>\n",
       "      <td>332614</td>\n",
       "      <td>2</td>\n",
       "      <td>54939</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-28</td>\n",
       "      <td>332614</td>\n",
       "      <td>27850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1440796528638</td>\n",
       "      <td>823521</td>\n",
       "      <td>2</td>\n",
       "      <td>9198</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-28</td>\n",
       "      <td>823521</td>\n",
       "      <td>4619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1440795514114</td>\n",
       "      <td>35231</td>\n",
       "      <td>2</td>\n",
       "      <td>25284</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-28</td>\n",
       "      <td>35231</td>\n",
       "      <td>12729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1440794798805</td>\n",
       "      <td>135803</td>\n",
       "      <td>2</td>\n",
       "      <td>128129</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-28</td>\n",
       "      <td>135803</td>\n",
       "      <td>64733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       TIMESTAMP  VISITORID  EVENT  ITEMID  TRANSACTIONID     TS_DATE  \\\n",
       "0  1440795810142     922839      2  387334            NaN  2015-08-28   \n",
       "1  1440796866269     332614      2   54939            NaN  2015-08-28   \n",
       "2  1440796528638     823521      2    9198            NaN  2015-08-28   \n",
       "3  1440795514114      35231      2   25284            NaN  2015-08-28   \n",
       "4  1440794798805     135803      2  128129            NaN  2015-08-28   \n",
       "\n",
       "   VISITOR_ID  ITEM_ID  \n",
       "0      922839   195153  \n",
       "1      332614    27850  \n",
       "2      823521     4619  \n",
       "3       35231    12729  \n",
       "4      135803    64733  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snf_cleaned = session.table('EVENTS_DATA_CLEANED')\n",
    "snf_cleaned.limit(5).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c2cb11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = snf_cleaned.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37be9cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf93a667f8a941bf97553400278f7bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unique_pairs = data[['VISITORID','VISITOR_ID']].drop_duplicates()\n",
    "user_id_dict = dict(zip(unique_pairs['VISITOR_ID'], unique_pairs['VISITORID']))\n",
    "\n",
    "unique_pairs = data[['ITEMID','ITEM_ID']].drop_duplicates()\n",
    "item_id_dict = dict(zip(unique_pairs['ITEM_ID'], unique_pairs['ITEMID']))\n",
    "\n",
    "sparse_item_user = sparse.csr_matrix((data['EVENT'].astype(float), (data['ITEM_ID'], data['VISITOR_ID'])))\n",
    "sparse_user_item = sparse.csr_matrix((data['EVENT'].astype(float), (data['VISITOR_ID'], data['ITEM_ID'])))\n",
    "\n",
    "#Building the model\n",
    "model = implicit.als.AlternatingLeastSquares(factors=20, regularization=0.1, iterations=20)\n",
    "\n",
    "alpha_val = 40\n",
    "data_conf = (sparse_user_item * alpha_val).astype('double')\n",
    "\n",
    "model.fit(data_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1911027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[591869,\n",
       " 684514,\n",
       " 157772,\n",
       " 395323,\n",
       " 1265471,\n",
       " 139953,\n",
       " 961247,\n",
       " 1359334,\n",
       " 822060,\n",
       " 1284429,\n",
       " 867447,\n",
       " 366396,\n",
       " 348645,\n",
       " 102941,\n",
       " 95405,\n",
       " 504226,\n",
       " 1320204,\n",
       " 248679,\n",
       " 929346,\n",
       " 11576]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['VISITORID'][0:20].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df477bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 41790  67074 196786  81517 107226]\n",
      " [176924 196343  52380  97541  72728]\n",
      " [ 41633 156023 129504 225872 112671]\n",
      " [144481 178472 139958 157515 196675]\n",
      " [156023 146279  33030   8009  69916]\n",
      " [224330 161235  18735 220854  61945]\n",
      " [  2719 169261 186716  46393  42604]\n",
      " [229223 117958 116537 146632  60097]\n",
      " [ 78785  76432 113669 105748  83839]\n",
      " [225872  63517  49024  29708 171695]\n",
      " [ 14707 105748 120647  14652  33030]\n",
      " [ 71944   8009 127790 159546 156023]\n",
      " [127790 223822  83374 156023 171135]\n",
      " [ 76432 176924 113669  77598  78785]\n",
      " [105748 214446 159407 186150  14652]\n",
      " [  2719  21276 126959  16696  98124]\n",
      " [  2719  49200  56952 222448 112216]\n",
      " [  2719 173948  94720 193605   2004]\n",
      " [  8009  76432 160324 137993 150733]\n",
      " [ 94720 224530  15075 176924  38313]]\n"
     ]
    }
   ],
   "source": [
    "#Get Recommendations for all users\n",
    "users = data['VISITORID'][0:20].to_list()\n",
    "recommended = model.recommend(users, sparse_user_item[users], N=5)\n",
    "recommended[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84f6e5c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'row_0': [41790, 67074, 196786, 81517, 107226],\n",
       " 'row_1': [176924, 196343, 52380, 97541, 72728],\n",
       " 'row_2': [41633, 156023, 129504, 225872, 112671],\n",
       " 'row_3': [144481, 178472, 139958, 157515, 196675],\n",
       " 'row_4': [156023, 146279, 33030, 8009, 69916],\n",
       " 'row_5': [224330, 161235, 18735, 220854, 61945],\n",
       " 'row_6': [2719, 169261, 186716, 46393, 42604],\n",
       " 'row_7': [229223, 117958, 116537, 146632, 60097],\n",
       " 'row_8': [78785, 76432, 113669, 105748, 83839],\n",
       " 'row_9': [225872, 63517, 49024, 29708, 171695],\n",
       " 'row_10': [14707, 105748, 120647, 14652, 33030],\n",
       " 'row_11': [71944, 8009, 127790, 159546, 156023],\n",
       " 'row_12': [127790, 223822, 83374, 156023, 171135],\n",
       " 'row_13': [76432, 176924, 113669, 77598, 78785],\n",
       " 'row_14': [105748, 214446, 159407, 186150, 14652],\n",
       " 'row_15': [2719, 21276, 126959, 16696, 98124],\n",
       " 'row_16': [2719, 49200, 56952, 222448, 112216],\n",
       " 'row_17': [2719, 173948, 94720, 193605, 2004],\n",
       " 'row_18': [8009, 76432, 160324, 137993, 150733],\n",
       " 'row_19': [94720, 224530, 15075, 176924, 38313]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommended_dict = {f\"row_{i}\": row.tolist() for i, row in enumerate(recommended[0])}\n",
    "recommended_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0b8f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Recommendations\n",
    "user_id = 2\n",
    "reco = model.recommend(user_id, sparse_user_item[user_id], N=5)\n",
    "print(reco)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65313e4f",
   "metadata": {},
   "source": [
    "# Testing Model Save and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4486cf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import cachetools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf121ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from scipy.sparse import save_npz\n",
    "from scipy.sparse import load_npz\n",
    "\n",
    "# Serialize user_id_dict\n",
    "with open('user_id_dict.json', 'w') as file:\n",
    "    json.dump(user_id_dict, file)\n",
    "\n",
    "# Serialize item_id_dict\n",
    "with open('item_id_dict.json', 'w') as file:\n",
    "    json.dump(item_id_dict, file)\n",
    "\n",
    "# Serialize sparse_user_item\n",
    "save_npz('sparse_user_item.npz', sparse_user_item)\n",
    "\n",
    "# Save model file\n",
    "FILE_LOCATION = 'als_model.joblib'\n",
    "joblib.dump(model, FILE_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ea623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = joblib.load('als_model.joblib')\n",
    "# Load user_id_dict\n",
    "with open('user_id_dict.json', 'r') as file:\n",
    "    user_id_dict2 = json.load(file)\n",
    "\n",
    "# Load item_id_dict\n",
    "with open('item_id_dict.json', 'r') as file:\n",
    "    item_id_dict2 = json.load(file)\n",
    "\n",
    "sparse_user_item2 = load_npz('sparse_user_item.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e50dfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Recommendations\n",
    "user_id = 2\n",
    "reco = model2.recommend(user_id, sparse_user_item2[user_id], N=5)\n",
    "print(reco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d7bd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(reco[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79484e7",
   "metadata": {},
   "source": [
    "# Option 1: SPROC Based Orchestration in Snowflake\n",
    "\n",
    "Here we take all the pieces of code written above for local testing and package in a modularized format. We may also choose to\n",
    "schedule the preprocess pipeline as a predecessor to model train and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a92a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.core import Root\n",
    "from snowflake.core.task import StoredProcedureCall, Task\n",
    "from snowflake.core.task.dagv1 import DAGOperation, DAG, DAGTask\n",
    "api_root = Root(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0386b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Preprocess Data\n",
    "def preprocess_data(session:Session) -> str:\n",
    "    datapath= 'RECOMMENDER_SYSTEMS.COLLABORATIVE_FILTERING_ALS.EVENTS_DATA'\n",
    "    snf_df = session.table(datapath)\n",
    "    snf_df = snf_df.with_column(\"TS_DATE\", F.to_date(F.to_timestamp(F.col('\"timestamp\"')/F.lit(1000))))\n",
    "    snf_df = snf_df.sort(F.col('\"TS_DATE\"').asc())\n",
    "\n",
    "    data = snf_df.to_pandas()\n",
    "    data['visitorid'] = data['visitorid'].astype(\"category\")\n",
    "    data['visitor_id'] = data['visitorid'].cat.codes\n",
    "    data['visitorid'] = data['visitorid'].astype(\"int\")\n",
    "\n",
    "    data['itemid'] = data['itemid'].astype(\"category\")\n",
    "    data['item_id'] = data['itemid'].cat.codes\n",
    "    data['itemid'] = data['itemid'].astype(\"int\")\n",
    "\n",
    "    data['event']= data['event'].astype('category')\n",
    "    data['event']= data['event'].cat.codes\n",
    "    data['event']= data['event'].astype('int')\n",
    "\n",
    "    session.write_pandas(data, table_name='EVENTS_DATA_CLEANED', auto_create_table=True, overwrite=True)\n",
    "\n",
    "    return 'DATA PROCESSING SUCCESS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abede6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Model Train + Inference\n",
    "def train_model(session:Session, sparse_user_item):\n",
    "    import implicit\n",
    "    import scipy.sparse as sparse\n",
    "\n",
    "    model = implicit.als.AlternatingLeastSquares(factors=20, regularization=0.1, iterations=20)\n",
    "    alpha_val = 40\n",
    "    data_conf = (sparse_user_item * alpha_val).astype('double')\n",
    "    model.fit(data_conf)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Clean output\n",
    "def process_output(session:Session, reco, user_id_dict, item_id_dict):\n",
    "    rec_df = pd.DataFrame(reco[0], columns=['rec1', 'rec2', 'rec3', 'rec4', 'rec5'])\n",
    "    user_df = pd.DataFrame(data = list(user_id_dict.items()), columns = ['visitor_id','visitorid'])\n",
    "    joined_df = user_df.join(rec_df)\n",
    "\n",
    "    for col in ['rec1', 'rec2', 'rec3', 'rec4', 'rec5']:\n",
    "        joined_df[col] = joined_df[col].map(item_id_dict)\n",
    "    \n",
    "    joined_df['PRED_TIMESTAMP'] = str(pd.Timestamp.now())\n",
    "    return joined_df\n",
    "\n",
    "# Run the Process\n",
    "def get_predictions(session:Session) -> str:\n",
    "    import pandas as pd\n",
    "    import implicit\n",
    "    import scipy.sparse as sparse\n",
    "    from datetime import datetime\n",
    "    import snowflake.snowpark.functions as F\n",
    "\n",
    "    data = session.table('RECOMMENDER_SYSTEMS.COLLABORATIVE_FILTERING_ALS.EVENTS_DATA_CLEANED').to_pandas()\n",
    "\n",
    "    # Map cat codes to visitorid\n",
    "    unique_pairs = data[['visitorid','visitor_id']].drop_duplicates()\n",
    "    user_id_dict = dict(zip(unique_pairs['visitor_id'], unique_pairs['visitorid']))\n",
    "\n",
    "    # Map cat codes to itemid\n",
    "    unique_pairs = data[['itemid','item_id']].drop_duplicates()\n",
    "    item_id_dict = dict(zip(unique_pairs['item_id'], unique_pairs['itemid']))\n",
    "\n",
    "    # Sparse matrix are more performant when the range of numbers isnt too large\n",
    "    # sparse_item_user = sparse.csr_matrix((data['event'].astype(float), (data['item_id'], data['visitor_id'])))\n",
    "    sparse_user_item = sparse.csr_matrix((data['event'].astype(float), (data['visitor_id'], data['item_id'])))\n",
    "    model = train_model(session, sparse_user_item)\n",
    "    recommended = model.recommend(list(user_id_dict.keys()), sparse_user_item[list(user_id_dict.keys())], N=5)\n",
    "\n",
    "    # Map users and items back to original codes\n",
    "    cleaned_df = process_output(session, recommended, user_id_dict, item_id_dict)\n",
    "    \n",
    "    # Save Data in Snowflake\n",
    "    session.write_pandas(cleaned_df, table_name='ITEM_RECOMMENDATIONS', auto_create_table=True, overwrite=True)\n",
    "\n",
    "    return 'Success'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717bd4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = api_root.databases['RECOMMENDER_SYSTEMS'].schemas['COLLABORATIVE_FILTERING_ALS']\n",
    "tasks = schema.tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9891233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Task 1: Preprocess Task\n",
    "task1_entity = Task(\n",
    "    \"PREPROCESS_DATA\",\n",
    "    definition = StoredProcedureCall(preprocess_data,\n",
    "                                   stage_location=\"@ML_MODELS\",\n",
    "                                   packages=[\"snowflake-snowpark-python\",\"snowflake-ml-python\", \"regex\"]),\n",
    "    warehouse = connection_parameters['warehouse'],\n",
    "    schedule = timedelta(days=1))\n",
    "\n",
    "task1 = tasks.create(task1_entity, mode=\"orReplace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e45be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Task 2: for model training and inference\n",
    "task2_entity = Task(\n",
    "    \"RECO_ENGINE\",\n",
    "    definition = StoredProcedureCall(get_predictions, stage_location=\"@ML_MODELS\", \n",
    "                                     packages=[\"snowflake-snowpark-python\",\"snowflake-ml-python\",\n",
    "                                               \"regex\", \"scipy\", \"implicit==0.6.2\", \"numpy==1.23.5\"]),\n",
    "    warehouse = connection_parameters['warehouse']\n",
    "    )\n",
    "\n",
    "task2_entity.predecessors = [\"RECOMMENDER_SYSTEMS.COLLABORATIVE_FILTERING_ALS.PREPROCESS_DATA\"]\n",
    "task2 = tasks.create(task2_entity, mode=\"orReplace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9667dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "task2.resume()\n",
    "task1.resume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c03fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "task1.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5dc1d9",
   "metadata": {},
   "source": [
    "# Option 2: Distributed Modeling using UDTF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0b79b0",
   "metadata": {},
   "source": [
    "### Model Training using SPROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cb50cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training SPROC\n",
    "def model_train(session:Session, table_name: str) -> str:\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    import joblib\n",
    "    import implicit\n",
    "    import scipy.sparse as sparse\n",
    "    from scipy.sparse import save_npz\n",
    "    from datetime import datetime\n",
    "\n",
    "    data = session.table(table_name).to_pandas()\n",
    "\n",
    "    # Map cat codes to visitorid\n",
    "    unique_pairs = data[['VISITORID','VISITOR_ID']].drop_duplicates()\n",
    "    user_id_dict = dict(zip(unique_pairs['VISITOR_ID'], unique_pairs['VISITORID']))\n",
    "\n",
    "    # Map cat codes to itemid\n",
    "    unique_pairs = data[['ITEMID','ITEM_ID']].drop_duplicates()\n",
    "    item_id_dict = dict(zip(unique_pairs['ITEM_ID'], unique_pairs['ITEMID']))\n",
    "\n",
    "    # Sparse matrix are more performant when the range of numbers isnt too large\n",
    "    # sparse_item_user = sparse.csr_matrix((data['event'].astype(float), (data['item_id'], data['visitor_id'])))\n",
    "    sparse_user_item = sparse.csr_matrix((data['EVENT'].astype(float), (data['VISITOR_ID'], data['ITEM_ID'])))\n",
    "\n",
    "    # Model Training\n",
    "    model = implicit.als.AlternatingLeastSquares(factors=20, regularization=0.1, iterations=20)\n",
    "    alpha_val = 40\n",
    "    data_conf = (sparse_user_item * alpha_val).astype('double')\n",
    "    model.fit(data_conf)\n",
    "\n",
    "    # Serialize user_id_dict        \n",
    "    with open('/tmp/user_id_dict.pkl', 'wb') as file:\n",
    "        joblib.dump(user_id_dict, file)\n",
    "        session.file.put('/tmp/user_id_dict.pkl', '@ML_MODELS/TRAIN_OUTPUT', auto_compress=False, overwrite=True)\n",
    "\n",
    "    # Serialize item_id_dict\n",
    "    with open('/tmp/item_id_dict.pkl', 'wb') as file:\n",
    "        joblib.dump(item_id_dict, file)\n",
    "        session.file.put('/tmp/item_id_dict.pkl', '@ML_MODELS/TRAIN_OUTPUT', auto_compress=False, overwrite=True)\n",
    "\n",
    "    # Serialize sparse_user_item\n",
    "    save_npz('/tmp/sparse_user_item.npz', sparse_user_item)\n",
    "    session.file.put('/tmp/sparse_user_item.npz', '@ML_MODELS/TRAIN_OUTPUT', auto_compress=False, overwrite=True)\n",
    "\n",
    "    # Save model file\n",
    "    FILE_LOCATION = '/tmp/als_model.joblib'\n",
    "    joblib.dump(model, FILE_LOCATION)\n",
    "    session.file.put(FILE_LOCATION, '@ML_MODELS/TRAIN_OUTPUT', auto_compress=False, overwrite=True)\n",
    "    \n",
    "    return 'Success'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e593246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registering the function as a Stored Procedure\n",
    "sproc_model_train = session.sproc.register(func=model_train,\n",
    "                                           name='ALS_MODEL_TRAIN',\n",
    "                                           is_permanent=True,\n",
    "                                           replace=True,\n",
    "                                           stage_location='@ML_MODELS',\n",
    "                                           packages=[\"snowflake-snowpark-python\",\"snowflake-ml-python\", \"joblib\",\n",
    "                                                     \"regex\", \"scipy\", \"implicit==0.6.2\", \"numpy==1.23.5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3014adcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'RECOMMENDER_SYSTEMS.COLLABORATIVE_FILTERING_ALS.EVENTS_DATA_CLEANED'\n",
    "sproc_model_train(session, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a035cf0",
   "metadata": {},
   "source": [
    "### Model Inference using SPROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c9a8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training SPROC\n",
    "def model_predict(session:Session) -> str:\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    import sys\n",
    "    import joblib\n",
    "    import implicit\n",
    "    import scipy.sparse as sparse\n",
    "    from scipy.sparse import load_npz\n",
    "\n",
    "    session.file.get(f\"@ML_MODELS/TRAIN_OUTPUT/als_model.joblib\", \"/tmp/\")\n",
    "    session.file.get(f\"@ML_MODELS/TRAIN_OUTPUT/sparse_user_item.npz\", \"/tmp/\")\n",
    "    session.file.get(f\"@ML_MODELS/TRAIN_OUTPUT/user_id_dict.pkl\", \"/tmp/\")\n",
    "\n",
    "    # Load Model\n",
    "    model = joblib.load(\"/tmp/als_model.joblib\")\n",
    "\n",
    "    # Load the Sparse input file\n",
    "    sparse_user_item = load_npz(\"/tmp/sparse_user_item.npz\")\n",
    "\n",
    "    with open(\"/tmp/user_id_dict.pkl\", 'rb') as file:\n",
    "        user_id_dict = joblib.load(file)\n",
    "\n",
    "    recommended = model.recommend(list(user_id_dict.keys()), sparse_user_item[list(user_id_dict.keys())], N=5)\n",
    "    \n",
    "    return len(recommended[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6f4a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registering the function as a Stored Procedure\n",
    "sproc_predict = session.sproc.register(func=model_predict,\n",
    "                                       name='ALS_PREDICT',\n",
    "                                       is_permanent=True,\n",
    "                                       replace=True,\n",
    "                                       stage_location='@ML_MODELS',\n",
    "                                    #    imports=['@ML_MODELS/TRAIN_OUTPUT/als_model.joblib',\n",
    "                                    #             '@ML_MODELS/TRAIN_OUTPUT/sparse_user_item.npz',\n",
    "                                    #             '@ML_MODELS/TRAIN_OUTPUT/user_id_dict.json'],\n",
    "                                       packages=[\"snowflake-snowpark-python\",\"snowflake-ml-python\", \"joblib\",\n",
    "                                                 \"regex\", \"scipy\", \"implicit==0.6.2\", \"numpy==1.23.5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8c399f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sproc_predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bf2210",
   "metadata": {},
   "source": [
    "### UDF for Infernece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6826392c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['VISITORID','VISITOR_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dfeb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple scoring function\n",
    "from cachetools import cached\n",
    "\n",
    "@cached(cache={})\n",
    "def load_from_stage(import_dir) -> object:\n",
    "    import joblib\n",
    "    from scipy.sparse import load_npz\n",
    "\n",
    "    # Load Model\n",
    "    file_loc = import_dir + 'als_model.joblib'\n",
    "    model = joblib.load(file_loc)\n",
    "\n",
    "    # Load the Sparse input file\n",
    "    file_loc = import_dir + 'sparse_user_item.npz'\n",
    "    sparse_user_item = load_npz(file_loc)\n",
    "\n",
    "    return model, sparse_user_item #, user_id_dict, item_id_dict, \n",
    "\n",
    "def udf_als_score(df: pd.DataFrame) -> pd.Series:\n",
    "    import os\n",
    "    import sys\n",
    "    import numpy\n",
    "    import json\n",
    "    import implicit\n",
    "    import scipy.sparse as sparse\n",
    "    from scipy.sparse import load_npz\n",
    "\n",
    "    # file-dependencies of UDFs are available in snowflake_import_directory\n",
    "    IMPORT_DIRECTORY_NAME = \"snowflake_import_directory\"\n",
    "    import_dir = sys._xoptions[IMPORT_DIRECTORY_NAME]\n",
    "\n",
    "    model, sparse_user_item = load_from_stage(import_dir)\n",
    "    df.columns = feature_cols\n",
    "    recommended = model.recommend(df['VISITORID'].to_list(), sparse_user_item[df['VISITORID'].to_list()], N=5)\n",
    "\n",
    "    def join_row_elements(row):\n",
    "        return ', '.join(map(str, row))\n",
    "    joined_arr = np.apply_along_axis(join_row_elements, 1, recommended[0])\n",
    "    return pd.Series(joined_arr)\n",
    "\n",
    "    # output = [len(recommended[0])]*len(recommended[0])\n",
    "    # return pd.Series(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d4080e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple scoring function\n",
    "from cachetools import cached\n",
    "\n",
    "@cached(cache={})\n",
    "def load_from_stage(import_dir) -> object:\n",
    "    import joblib\n",
    "    from scipy.sparse import load_npz\n",
    "\n",
    "    # Load Model\n",
    "    file_loc = import_dir + 'als_model.joblib'\n",
    "    model = joblib.load(file_loc)\n",
    "\n",
    "    # Load the Sparse input file\n",
    "    file_loc = import_dir + 'sparse_user_item.npz'\n",
    "    sparse_user_item = load_npz(file_loc)\n",
    "\n",
    "    return model, sparse_user_item #, user_id_dict, item_id_dict, \n",
    "\n",
    "def udf_als_score(df: T.PandasDataFrame[int, int]) -> T.PandasSeries[dict]:\n",
    "    import os\n",
    "    import sys\n",
    "    import numpy\n",
    "    import json\n",
    "    import implicit\n",
    "    import scipy.sparse as sparse\n",
    "    from scipy.sparse import load_npz\n",
    "\n",
    "    # file-dependencies of UDFs are available in snowflake_import_directory\n",
    "    IMPORT_DIRECTORY_NAME = \"snowflake_import_directory\"\n",
    "    import_dir = sys._xoptions[IMPORT_DIRECTORY_NAME]\n",
    "\n",
    "    model, sparse_user_item = load_from_stage(import_dir)\n",
    "    df.columns = feature_cols\n",
    "    recommended = model.recommend(df['VISITORID'].to_list(), sparse_user_item[df['VISITORID'].to_list()], N=5)\n",
    "    \n",
    "    # Processing for output\n",
    "    recommended_series = pd.Series([{\"recommendations\": row.tolist()} for row in recommended[0]])\n",
    "\n",
    "    return recommended_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1b8bed7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='Statement executed successfully.')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql('USE WAREHOUSE SSK_RESEARCH').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7fca743e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The version of package 'joblib' in the local environment is 1.3.2, which does not fit the criteria for the requirement 'joblib'. Your UDF might not work when the package version is different between the server and your local environment.\n"
     ]
    }
   ],
   "source": [
    "# Register UDF\n",
    "udf_als = session.udf.register(func=udf_als_score, \n",
    "                               name=\"ALS_COLAB_FILTERING\", \n",
    "                               stage_location='@ML_MODELS',\n",
    "                               replace=True,\n",
    "                               is_permanent=True, \n",
    "                               imports=['@ML_MODELS/TRAIN_OUTPUT/als_model.joblib',\n",
    "                                        '@ML_MODELS/TRAIN_OUTPUT/sparse_user_item.npz'],\n",
    "                               packages=[\"snowflake-snowpark-python\",\"snowflake-ml-python\", \"joblib\",\n",
    "                                         \"regex\", \"scipy\", \"implicit==0.6.2\", \"numpy==1.23.5\"], \n",
    "                               session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "424a1e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowdf_test = session.table('RECOMMENDER_SYSTEMS.COLLABORATIVE_FILTERING_ALS.EVENTS_DATA_CLEANED')\n",
    "feature_cols = ['VISITORID','VISITOR_ID']\n",
    "\n",
    "test_sdf_w_preds = snowdf_test.with_column('PREDICTED',\n",
    "                                           F.call_udf(\"ALS_COLAB_FILTERING\", [F.col(c) for c in feature_cols]))\n",
    "\n",
    "test_sdf_w_preds.write.mode(\"overwrite\").save_as_table(\"RECOMMENDATIONS_OUTPUT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2e2f0149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>VISITORID</th>\n",
       "      <th>EVENT</th>\n",
       "      <th>ITEMID</th>\n",
       "      <th>TRANSACTIONID</th>\n",
       "      <th>TS_DATE</th>\n",
       "      <th>VISITOR_ID</th>\n",
       "      <th>ITEM_ID</th>\n",
       "      <th>PREDICTED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1440797505309</td>\n",
       "      <td>638482</td>\n",
       "      <td>2</td>\n",
       "      <td>156587</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-28</td>\n",
       "      <td>638482</td>\n",
       "      <td>78989</td>\n",
       "      <td>{\\n  \"recommendations\": [\\n    172025,\\n    19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1440792487754</td>\n",
       "      <td>1368001</td>\n",
       "      <td>2</td>\n",
       "      <td>110243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-28</td>\n",
       "      <td>1368001</td>\n",
       "      <td>55677</td>\n",
       "      <td>{\\n  \"recommendations\": [\\n    113669,\\n    76...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1440792476356</td>\n",
       "      <td>671482</td>\n",
       "      <td>2</td>\n",
       "      <td>461177</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-28</td>\n",
       "      <td>671482</td>\n",
       "      <td>232237</td>\n",
       "      <td>{\\n  \"recommendations\": [\\n    38313,\\n    148...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1440793449724</td>\n",
       "      <td>973751</td>\n",
       "      <td>2</td>\n",
       "      <td>109948</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-28</td>\n",
       "      <td>973751</td>\n",
       "      <td>55526</td>\n",
       "      <td>{\\n  \"recommendations\": [\\n    159407,\\n    21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1440792901632</td>\n",
       "      <td>1124131</td>\n",
       "      <td>2</td>\n",
       "      <td>370444</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-28</td>\n",
       "      <td>1124131</td>\n",
       "      <td>186625</td>\n",
       "      <td>{\\n  \"recommendations\": [\\n    212797,\\n    15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756096</th>\n",
       "      <td>1440797997292</td>\n",
       "      <td>1261748</td>\n",
       "      <td>2</td>\n",
       "      <td>367956</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-28</td>\n",
       "      <td>1261748</td>\n",
       "      <td>185411</td>\n",
       "      <td>{\\n  \"recommendations\": [\\n    110536,\\n    19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756097</th>\n",
       "      <td>1440795591479</td>\n",
       "      <td>569539</td>\n",
       "      <td>2</td>\n",
       "      <td>320130</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-28</td>\n",
       "      <td>569539</td>\n",
       "      <td>161235</td>\n",
       "      <td>{\\n  \"recommendations\": [\\n    14652,\\n    676...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756098</th>\n",
       "      <td>1440797638044</td>\n",
       "      <td>24899</td>\n",
       "      <td>2</td>\n",
       "      <td>209855</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-28</td>\n",
       "      <td>24899</td>\n",
       "      <td>105681</td>\n",
       "      <td>{\\n  \"recommendations\": [\\n    150148,\\n    85...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756099</th>\n",
       "      <td>1440795039186</td>\n",
       "      <td>342436</td>\n",
       "      <td>2</td>\n",
       "      <td>354745</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-28</td>\n",
       "      <td>342436</td>\n",
       "      <td>178753</td>\n",
       "      <td>{\\n  \"recommendations\": [\\n    118428,\\n    58...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756100</th>\n",
       "      <td>1440796214385</td>\n",
       "      <td>931234</td>\n",
       "      <td>2</td>\n",
       "      <td>364032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-28</td>\n",
       "      <td>931234</td>\n",
       "      <td>183464</td>\n",
       "      <td>{\\n  \"recommendations\": [\\n    24831,\\n    176...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2756101 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             TIMESTAMP  VISITORID  EVENT  ITEMID  TRANSACTIONID     TS_DATE  \\\n",
       "0        1440797505309     638482      2  156587            NaN  2015-08-28   \n",
       "1        1440792487754    1368001      2  110243            NaN  2015-08-28   \n",
       "2        1440792476356     671482      2  461177            NaN  2015-08-28   \n",
       "3        1440793449724     973751      2  109948            NaN  2015-08-28   \n",
       "4        1440792901632    1124131      2  370444            NaN  2015-08-28   \n",
       "...                ...        ...    ...     ...            ...         ...   \n",
       "2756096  1440797997292    1261748      2  367956            NaN  2015-08-28   \n",
       "2756097  1440795591479     569539      2  320130            NaN  2015-08-28   \n",
       "2756098  1440797638044      24899      2  209855            NaN  2015-08-28   \n",
       "2756099  1440795039186     342436      2  354745            NaN  2015-08-28   \n",
       "2756100  1440796214385     931234      2  364032            NaN  2015-08-28   \n",
       "\n",
       "         VISITOR_ID  ITEM_ID  \\\n",
       "0            638482    78989   \n",
       "1           1368001    55677   \n",
       "2            671482   232237   \n",
       "3            973751    55526   \n",
       "4           1124131   186625   \n",
       "...             ...      ...   \n",
       "2756096     1261748   185411   \n",
       "2756097      569539   161235   \n",
       "2756098       24899   105681   \n",
       "2756099      342436   178753   \n",
       "2756100      931234   183464   \n",
       "\n",
       "                                                 PREDICTED  \n",
       "0        {\\n  \"recommendations\": [\\n    172025,\\n    19...  \n",
       "1        {\\n  \"recommendations\": [\\n    113669,\\n    76...  \n",
       "2        {\\n  \"recommendations\": [\\n    38313,\\n    148...  \n",
       "3        {\\n  \"recommendations\": [\\n    159407,\\n    21...  \n",
       "4        {\\n  \"recommendations\": [\\n    212797,\\n    15...  \n",
       "...                                                    ...  \n",
       "2756096  {\\n  \"recommendations\": [\\n    110536,\\n    19...  \n",
       "2756097  {\\n  \"recommendations\": [\\n    14652,\\n    676...  \n",
       "2756098  {\\n  \"recommendations\": [\\n    150148,\\n    85...  \n",
       "2756099  {\\n  \"recommendations\": [\\n    118428,\\n    58...  \n",
       "2756100  {\\n  \"recommendations\": [\\n    24831,\\n    176...  \n",
       "\n",
       "[2756101 rows x 9 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_df = test_sdf_w_preds.to_pandas()\n",
    "local_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9069dbc",
   "metadata": {},
   "source": [
    "### Orchestration Using Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3d994e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Preprocess Data\n",
    "def train_and_predict(session:Session) -> str:\n",
    "    from snowflake.snowpark.functions import udf\n",
    "    import snowflake.snowpark.functions as F\n",
    "\n",
    "    # Call SPROC\n",
    "    table_name = 'RECOMMENDER_SYSTEMS.COLLABORATIVE_FILTERING_ALS.EVENTS_DATA_CLEANED'\n",
    "    _ = sproc_model_train(session, table_name)\n",
    "\n",
    "    # Prediction using UDF\n",
    "    snowdf_test = session.table('table_name')\n",
    "    feature_cols = ['VISITORID','VISITOR_ID']\n",
    "    test_sdf_w_preds = snowdf_test.with_column('PREDICTED',\n",
    "                                               F.call_udf(\"ALS_COLAB_FILTERING\", [F.col(c) for c in feature_cols]))\n",
    "\n",
    "    test_sdf_w_preds.write.mode(\"overwrite\").save_as_table(\"RECOMMENDATIONS_OUTPUT\")\n",
    "\n",
    "    return 'Recommendation Model Success'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "47975b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_root = Root(session)\n",
    "schema = api_root.databases['RECOMMENDER_SYSTEMS'].schemas['COLLABORATIVE_FILTERING_ALS']\n",
    "tasks = schema.tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9e2355de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Task 1: Preprocess Task\n",
    "task1_entity = Task(\n",
    "    \"TRAIN_AND_PREDICT\",\n",
    "    definition = StoredProcedureCall(train_and_predict,\n",
    "                                   stage_location=\"@ML_MODELS\",\n",
    "                                   packages=[\"snowflake-snowpark-python\",\"snowflake-ml-python\", \"regex\"]),\n",
    "    warehouse = connection_parameters['warehouse'],\n",
    "    schedule = timedelta(days=1))\n",
    "\n",
    "task1 = tasks.create(task1_entity, mode=\"orReplace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "991d8e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "task1.resume()\n",
    "task1.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d829078",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
