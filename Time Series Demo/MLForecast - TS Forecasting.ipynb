{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bea36370",
   "metadata": {},
   "source": [
    "# Steps in this Notebook\n",
    "\n",
    "1. Imports\n",
    "2. Snowflake Setup\n",
    "3. Local testing wit MLForecast\n",
    "4. Snowflake testing with MLForecast using UDTF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d72538",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb04fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.session import Session\n",
    "import snowflake.snowpark.types as T\n",
    "import snowflake.snowpark.functions as F\n",
    "from snowflake.snowpark.functions import col\n",
    "\n",
    "from snowflake.snowpark.functions import udf\n",
    "from snowflake.snowpark.types import IntegerType, FloatType, StringType,StructType, StructField\n",
    "\n",
    "import snowflake.ml.modeling.preprocessing as snowml\n",
    "from snowflake.ml.modeling.xgboost import XGBClassifier\n",
    "from snowflake.ml.modeling.preprocessing import KBinsDiscretizer, OrdinalEncoder, OneHotEncoder\n",
    "from snowflake.ml.modeling.impute import SimpleImputer\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import date, timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc080c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_parameters = json.load(open('/Users/skhara/Documents/Code/creds.json'))\n",
    "session = Session.builder.configs(connection_parameters).create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0697a78d-dfe2-4b7d-92a0-ae48c7053b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daf6745",
   "metadata": {},
   "source": [
    "# Snowflake Setup: Create a Database and Schema\n",
    "\n",
    "We will be using PUBLIC schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463a699a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TS data from Store_Traffic Database into ACCRUENT_TS_FORECASTING DB for testing purposes.\n",
    "sdf_raw = session.table('TIME_SERIES_1K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6ff284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Number of Time Series/Pumps that we have to predict\n",
    "session.sql('SELECT COUNT(DISTINCT SERIES_ID) FROM TIME_SERIES_1K').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b5e478",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the SERIES_ID and Start and End date of Time Series\n",
    "session.sql('SELECT SERIES_ID, MIN(DATE), MAX(DATE) FROM TIME_SERIES_1K GROUP BY SERIES_ID').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed6bf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_raw.limit(5).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdff238d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_raw.describe().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46caa133",
   "metadata": {},
   "source": [
    "# Local Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cd657a-7f5e-4ee9-b766-b1238876a80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = sdf_raw.filter((F.col(\"SERIES_ID\") == 62)).to_pandas()\n",
    "\n",
    "# Here onwards copy paste in UDTF\n",
    "df_data['DATE'] = pd.to_datetime(df_data['DATE'])\n",
    "df_data.groupby('DATE').sum('VALUE').reset_index()\n",
    "# df_data = df_data[['DATE','VALUE']]\n",
    "df_data = df_data.sort_values(by=['DATE']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573f853f-b8ba-4112-87c2-77d71517a44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84792e4-0075-4c5d-9cb0-e2d22f8f635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.set_index('DATE')['VALUE'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eb70ed-8a82-4297-b70a-a2bcd480e38d",
   "metadata": {},
   "source": [
    "### Testing with Nixtla MLForecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763ae7c3-4bfe-4803-8470-8d19fe7c8d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mlf = df_data.copy()\n",
    "df_mlf.columns = ['ds', 'unique_id', 'y']\n",
    "df_mlf.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c45cc91-93ea-43d7-a80b-e8fe4eb23877",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlforecast import MLForecast\n",
    "from mlforecast.target_transforms import Differences\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe0ecee-6cd4-4a8e-9d39-fc509444b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fc40b6-f0e0-4466-9307-55d383d241a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fh = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2b4c76-8573-457f-98eb-06ec152dc948",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = MLForecast(\n",
    "    models=[LinearRegression(),XGBRegressor()],\n",
    "    freq='D',  # our serie has a monthly frequency\n",
    "    lags=[1,7,28,60],\n",
    "    target_transforms=[Differences([1])],\n",
    ")\n",
    "fcst.fit(df_mlf.iloc[0:-fh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89c98f5-61dd-4d3b-a6da-5fcd8e8c1d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = fcst.predict(fh)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4a208a-d3be-4479-9d17-d9a9a317342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = preds.merge(df_mlf, left_on=['ds','unique_id'], right_on = ['ds','unique_id'],how='left')\n",
    "df_res = df_res.drop('unique_id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b1e9c0-a0ba-4107-ae9e-f542238b0e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_res.copy()\n",
    "df.set_index('ds', inplace=True)\n",
    "\n",
    "# Plotting the time series\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(df.index, df['LinearRegression'], label='Linear Regression')\n",
    "plt.plot(df.index, df['XGBRegressor'], label='XGB Regressor')\n",
    "plt.plot(df.index, df['y'], label='Actual Values')\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title('Time Series Plot')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Values')\n",
    "\n",
    "# Adding legend\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421bbc16-a9c4-41ea-86cc-8c8c612f65c2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Testing with Sktime Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3e96f4-cbcb-4a2a-9fa4-f76ab62f2796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "from sktime.forecasting.theta import ThetaForecaster\n",
    "from sktime.performance_metrics.forecasting import mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0823b9e-ac08-48cb-927d-18d5ed3253bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = pd.Series(df_data['VALUE'].values, index=df_data['TIMESTAMP'])\n",
    "df_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597f7c8d-f117-42ca-8329-586d2bd159f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input.index.freq = 'D'  # Set this to the appropriate frequency\n",
    "df_input = df_input.resample('D').asfreq()  # Resample if needed; adjust the 'M' if using a different frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3336a46f-ad1f-4789-8775-988b3e0e6040",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train, y_test = temporal_train_test_split(df_input)\n",
    "fh = ForecastingHorizon(y_test.index, is_relative=False)\n",
    "forecaster = ThetaForecaster(sp=12)  # monthly seasonal periodicity\n",
    "forecaster.fit(y_train)\n",
    "y_pred = forecaster.predict(fh)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2933a734-a66a-4d63-8dec-56457f27d3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = forecaster.predict(fh)\n",
    "mean_absolute_percentage_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b397dea7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Testing with Dart Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a28b048",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install darts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f610dee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import darts\n",
    "from darts import TimeSeries\n",
    "from darts.models import FFT\n",
    "from darts.metrics import mae, mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed10297",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train + Forecast Length\n",
    "train_length = 600\n",
    "forecast_horizon = 30\n",
    "train_end = max(df_data['TIMESTAMP']) - pd.Timedelta(days = 30)\n",
    "train_start = train_end - pd.Timedelta(days = 600)\n",
    "df_input = df_data.loc[(df_data['TIMESTAMP'] >= train_start) &\n",
    "                        (df_data['TIMESTAMP'] < train_end)].reset_index(drop=True)\n",
    "df_input = df_input.set_index('TIMESTAMP')\n",
    "df_input.index.name = 'time'\n",
    "\n",
    "ts_train = TimeSeries.from_dataframe(df_input, fill_missing_dates=True, freq='D')\n",
    "ts_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20a1f0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from darts.models import XGBModel\n",
    "my_model = XGBModel(lags = 10, n_estimators=100, max_depth=5)\n",
    "my_model.fit(ts_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39873efc-fbf0-486f-a0fc-639c6dbd9845",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ts_forecast = my_model.predict(forecast_horizon)\n",
    "data = ts_forecast.pd_dataframe().reset_index().values\n",
    "df_forecasted = pd.DataFrame(data, columns = ['TIMESTAMP','FORECAST'])\n",
    "df_forecasted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d711832d-c9a3-4424-a312-bfe5d7c0c637",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare = df_data.merge(df_forecasted, how='left', left_on='TIMESTAMP', right_on='TIMESTAMP')\n",
    "df_compare = df_compare.set_index('TIMESTAMP')\n",
    "df_compare.iloc[-90:].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90779e69-6cae-42fb-b9de-6c9bd766af21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = df_compare.dropna()\n",
    "df_res = (df_res['VALUE'] - df_res['FORECAST'])/df_res['VALUE'] * 100\n",
    "print('MAPE= ',df_res.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18fea84-265a-42af-8884-dece920c2d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99c7dd1c",
   "metadata": {},
   "source": [
    "# Creating UDTF for multi-node parallelized model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fdd949-b55b-41de-a25e-6405bea7a38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = T.StructType([\n",
    "    T.StructField(\"ID\", T.IntegerType()),\n",
    "    T.StructField(\"TIMESTAMP\", T.DateType()),\n",
    "    T.StructField(\"LINREG\", T.FloatType()),\n",
    "    T.StructField(\"XGB\", T.FloatType()),\n",
    "    T.StructField(\"TRAIN_START\", T.DateType()),\n",
    "    T.StructField(\"TRAIN_END\", T.DateType()),\n",
    "    T.StructField(\"FORECAST_HORIZON\", T.IntegerType())\n",
    "                  ])\n",
    "\n",
    "@F.udtf(output_schema = schema,\n",
    "        input_types = [T.VariantType()],\n",
    "        name = \"TSF_MLFORECAST\", is_permanent=True, stage_location= \"@DEMO_DB.PUBLIC.ML_MODELS\", session=session,\n",
    "        packages=['pandas', 'mlforecast' ,'xgboost', 'scikit-learn'],\n",
    "        replace=True\n",
    "       )\n",
    "\n",
    "class forecast:\n",
    "    def __init__(self):\n",
    "        self.rows=[]\n",
    "        self.dfs=[]\n",
    "    \n",
    "    def process(self, data):\n",
    "        self.rows.append(data)\n",
    "\n",
    "        # Merge rows into a dataframe\n",
    "        if len(self.rows) >= 16000:\n",
    "            df = pd.DataFrame(self.rows)\n",
    "            self.dfs.append(df)\n",
    "            self.rows = []\n",
    "        \n",
    "        # Merge dataframes into a single dataframe\n",
    "        # Minimizes memory footprint\n",
    "        if len(self.dfs) >= 100:\n",
    "            merged_df = pd.concat(self.dfs)\n",
    "            self.dfs = [merged_df]\n",
    "\n",
    "        yield None\n",
    "    \n",
    "    def end_partition(self):\n",
    "        # Merge any remaining rows\n",
    "        from mlforecast import MLForecast\n",
    "        from mlforecast.target_transforms import Differences\n",
    "        from xgboost import XGBRegressor\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "\n",
    "        if len(self.rows) > 0:\n",
    "            df = pd.DataFrame(self.rows)\n",
    "            self.dfs.append(df)\n",
    "            self.rows = []\n",
    "\n",
    "        # Process Input\n",
    "        df_input = pd.concat(self.dfs)\n",
    "        df_input['DATE'] = pd.to_datetime(df_input['DATE'])\n",
    "        df_input.groupby(['DATE','SERIES_ID']).sum('VALUE').reset_index()\n",
    "        df_input = df_input[['DATE','SERIES_ID','VALUE']]\n",
    "        df_input.columns = ['ds','unique_id','y']\n",
    "\n",
    "        #Train + Forecast Length\n",
    "        train_length = 600\n",
    "        fh = 30 # Forecast Horizon\n",
    "        train_end = max(df_input['ds'])\n",
    "        train_start = train_end - pd.Timedelta(days = 600)\n",
    "        \n",
    "        df_input = df_input.loc[(df_input['ds'] >= train_start) &\n",
    "                                (df_input['ds'] < train_end)].reset_index(drop=True)\n",
    "\n",
    "        \n",
    "        fcst = MLForecast(models=[LinearRegression(),XGBRegressor()],\n",
    "                          freq='D',\n",
    "                          lags=[1,7,28,60],\n",
    "                          target_transforms=[Differences([1])])\n",
    "        \n",
    "        fcst.fit(df_mlf.iloc[0:-fh])\n",
    "\n",
    "        ts_forecast = fcst.predict(fh)\n",
    "        \n",
    "        # Processing\n",
    "        ts_forecast.columns = ['ID','TIMESTAMP','LINREG','XGB']\n",
    "        ts_forecast['TRAIN_START'] = train_start\n",
    "        ts_forecast['TRAIN_END'] = train_end\n",
    "        ts_forecast['FORECAST_HORIZON'] = fh\n",
    "\n",
    "        yield from ts_forecast.itertuples(index=False, name=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e564cf-8057-4e90-af0e-2a71574a515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_raw.limit(5).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647ceb61-a055-4c6a-805a-b795fbd90f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = session.table('TIME_SERIES') \\\n",
    "        .with_column('ROW', F.object_construct_keep_null('*')) \\\n",
    "        .select(F.col('SERIES_ID'), F.col('ROW'))\n",
    "\n",
    "store_forecast_test = F.table_function(\"TSF_MLFORECAST\")\n",
    "\n",
    "variant_column = F.parse_json(df.col('ROW').cast(T.VariantType()))\n",
    "\n",
    "forecast = df.select(\n",
    "                F.col('SERIES_ID'), \n",
    "                store_forecast_test(variant_column).over(partition_by=['SERIES_ID'])\n",
    "                )\n",
    "\n",
    "forecast = forecast.with_column('FORECAST_DATETIME', F.current_timestamp())\n",
    "forecast.write.save_as_table(\"DEMO_DB.PUBLIC.FORECAST_USING_MLFORECAST\", mode=\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411e0f1f-202b-4bfd-9f40-c4d9b58ccb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_ref = session.table('DEMO_DB.PUBLIC.FORECAST_USING_MLFORECAST')\n",
    "sdf_ref.limit(5).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6e2d58-3021-45f4-ac8f-e3ad0c2bd8cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17284b91-e68d-4520-9cba-ec37aa4a5531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ef715fc",
   "metadata": {},
   "source": [
    "### Upload library to Snowflake Stage\n",
    "We are uploading to a stage as this library is not available through the Snowflake Anaconda Channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d58728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nfoursid\n",
    "nfoursid_path = nfoursid.__path__[0]\n",
    "print(nfoursid_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a6bcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import darts\n",
    "darts_path = darts.__path__[0]\n",
    "print(darts_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cfa018",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file_path = \"/Users/skhara/anaconda3/envs/pysnowpark_ml_tpcds/lib/python3.9/site-packages/nfoursid.zip\"\n",
    "session.file.put(zip_file_path, \"@ML_MODELS\", auto_compress=False, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca01956",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file_path = \"/Users/skhara/anaconda3/envs/pysnowpark_ml_tpcds/lib/python3.9/site-packages/darts.zip\"\n",
    "session.file.put(zip_file_path, \"@ML_MODELS\", auto_compress=False, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5c833d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "schema = T.StructType([\n",
    "    T.StructField(\"TIMESTAMP\", T.DateType()),\n",
    "    T.StructField(\"FORECAST\", T.IntegerType()),\n",
    "    T.StructField(\"TRAIN_START\", T.DateType()),\n",
    "    T.StructField(\"TRAIN_END\", T.DateType()),\n",
    "    T.StructField(\"FORECAST_HORIZON\", T.IntegerType())\n",
    "])\n",
    "\n",
    "@F.udtf(output_schema = schema,\n",
    "        input_types = [T.VariantType()],\n",
    "        name = \"PUMP_TS_DARTS_RANDOMFOREST\", is_permanent=True, stage_location=\"@ML_MODELS\", session=session,\n",
    "        packages=['pandas', 'fsspec==2023.4.0','holidays==0.18',\n",
    "                  'joblib==1.2.0','lightning-utilities==0.7.1','matplotlib==3.7.1',\n",
    "                  'plotly==5.9.0','pmdarima==2.0.3','pytorch==2.0.1',\n",
    "                  'pytorch-lightning==2.0.3','pyyaml==6.0','scikit-learn==1.2.2',\n",
    "                  'scipy==1.10.1','snowflake-snowpark-python==1.4.0','statsmodels',\n",
    "                  'tbats==1.1.3','torchmetrics==0.11.4','tqdm','xarray'\n",
    "                 ], replace=True,\n",
    "        imports = [\"@ML_MODELS/nfoursid.zip\", \"@ML_MODELS/darts.zip\"])\n",
    "\n",
    "class forecast:\n",
    "    def __init__(self):\n",
    "        self.rows=[]\n",
    "        self.dfs=[]\n",
    "    \n",
    "    def process(self, data):\n",
    "        self.rows.append(data)\n",
    "\n",
    "        # Merge rows into a dataframe\n",
    "        if len(self.rows) >= 16000:\n",
    "            df = pd.DataFrame(self.rows)\n",
    "            self.dfs.append(df)\n",
    "            self.rows = []\n",
    "        \n",
    "        # Merge dataframes into a single dataframe\n",
    "        # Minimizes memory footprint\n",
    "        if len(self.dfs) >= 100:\n",
    "            merged_df = pd.concat(self.dfs)\n",
    "            self.dfs = [merged_df]\n",
    "\n",
    "        yield None\n",
    "    \n",
    "    def end_partition(self):\n",
    "        # Merge any remaining rows\n",
    "        from darts import TimeSeries\n",
    "        from darts.models import RandomForest\n",
    "\n",
    "        if len(self.rows) > 0:\n",
    "            df = pd.DataFrame(self.rows)\n",
    "            self.dfs.append(df)\n",
    "            self.rows = []\n",
    "        \n",
    "        # Process Input\n",
    "        df_input = pd.concat(self.dfs)\n",
    "        df_input['TIMESTAMP'] = pd.to_datetime(df_input['TIMESTAMP'])\n",
    "        df_input = df_input.groupby('TIMESTAMP').sum('VALUE').reset_index()\n",
    "        df_input = df_input[['TIMESTAMP','VALUE']]\n",
    "\n",
    "        #Train + Forecast Length\n",
    "        train_length = 600\n",
    "        forecast_horizon = 30\n",
    "        train_end = max(df_input['TIMESTAMP'])\n",
    "        train_start = train_end - pd.Timedelta(days = 600)\n",
    "        \n",
    "        df_input = df_input.loc[(df_input['TIMESTAMP'] >= train_start) &\n",
    "                                (df_input['TIMESTAMP'] < train_end)].reset_index(drop=True)\n",
    "        df_input = df_input.set_index('TIMESTAMP')\n",
    "        df_input.index.name = 'time'\n",
    "        \n",
    "        # Convert DataFrame to Darts TS Object\n",
    "        ts_train = TimeSeries.from_dataframe(df_input, fill_missing_dates=True, freq='D')\n",
    "        \n",
    "        # Initialize Model\n",
    "        my_model = RandomForest(lags = 10, n_estimators=100, max_depth=5)\n",
    "\n",
    "        # Fit Model and Predict\n",
    "        my_model.fit(ts_train)\n",
    "        ts_forecast = my_model.predict(forecast_horizon)\n",
    "        \n",
    "        # Processing\n",
    "        data = ts_forecast.pd_dataframe().reset_index().values\n",
    "        df_forecast = pd.DataFrame(data, columns = ['TIMESTAMP','VALUE'])\n",
    "        df_forecast['TRAIN_START'] = train_start\n",
    "        df_forecast['TRAIN_END'] = train_end\n",
    "        df_forecast['FORECAST_HORIZON'] = forecast_horizon\n",
    "\n",
    "        yield from df_forecast.itertuples(index=False, name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d85095",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = session.table('TIME_SERIES_DATA') \\\n",
    "        .with_column('ROW', F.object_construct_keep_null('*')) \\\n",
    "        .select(F.col('PUMP_ID'), F.col('ROW'))\n",
    "\n",
    "store_forecast_test = F.table_function(\"PUMP_TS_DARTS_RANDOMFOREST\")\n",
    "\n",
    "variant_column = F.parse_json(df.col('ROW').cast(T.VariantType()))\n",
    "\n",
    "forecast = df.select(\n",
    "                F.col('PUMP_ID'),\n",
    "                F.col('GENDER')\n",
    "                store_forecast_test(variant_column).over(partition_by=['PUMP_ID','GENDER'])\n",
    "                )\n",
    "\n",
    "forecast = forecast.with_column('MODEL', F.lit('RANDOMFOREST'))\n",
    "forecast = forecast.with_column('FORECAST_DATETIME', F.current_timestamp())\n",
    "forecast.write.save_as_table(\"FORECAST_USING_DARTS_RANDOMFOREST\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eed478",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = session.table('FORECAST_USING_DARTS_RANDOMFOREST')\n",
    "df_temp.limit(5).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa871440",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = T.StructType([\n",
    "    T.StructField(\"TIMESTAMP\", T.DateType()),\n",
    "    T.StructField(\"FORECAST\", T.IntegerType()),\n",
    "    T.StructField(\"TRAIN_START\", T.DateType()),\n",
    "    T.StructField(\"TRAIN_END\", T.DateType()),\n",
    "    T.StructField(\"FORECAST_HORIZON\", T.IntegerType())\n",
    "])\n",
    "\n",
    "@F.udtf(output_schema = schema,\n",
    "        input_types = [T.VariantType()],\n",
    "        name = \"PUMP_TS_DARTS_FFT\", is_permanent=True, stage_location=\"@ML_MODELS\", session=session,\n",
    "        packages=['pandas', 'fsspec==2023.4.0','holidays==0.18',\n",
    "                  'joblib==1.2.0','lightning-utilities==0.7.1','matplotlib==3.7.1',\n",
    "                  'plotly==5.9.0','pmdarima==2.0.3','pytorch==2.0.1',\n",
    "                  'pytorch-lightning==2.0.3','pyyaml==6.0','scikit-learn==1.2.2',\n",
    "                  'scipy==1.10.1','snowflake-snowpark-python==1.4.0','statsmodels',\n",
    "                  'tbats==1.1.3','torchmetrics==0.11.4','tqdm','xarray'\n",
    "                 ], replace=True,\n",
    "        imports = [\"@ML_MODELS/nfoursid.zip\", \"@ML_MODELS/darts.zip\"])\n",
    "\n",
    "class forecast:\n",
    "    def __init__(self):\n",
    "        self.rows=[]\n",
    "        self.dfs=[]\n",
    "    \n",
    "    def process(self, data):\n",
    "        self.rows.append(data)\n",
    "\n",
    "        # Merge rows into a dataframe\n",
    "        if len(self.rows) >= 16000:\n",
    "            df = pd.DataFrame(self.rows)\n",
    "            self.dfs.append(df)\n",
    "            self.rows = []\n",
    "        \n",
    "        # Merge dataframes into a single dataframe\n",
    "        # Minimizes memory footprint\n",
    "        if len(self.dfs) >= 100:\n",
    "            merged_df = pd.concat(self.dfs)\n",
    "            self.dfs = [merged_df]\n",
    "\n",
    "        yield None\n",
    "    \n",
    "    def end_partition(self):\n",
    "        # Merge any remaining rows\n",
    "        from darts import TimeSeries\n",
    "        from darts.models import FFT\n",
    "\n",
    "        if len(self.rows) > 0:\n",
    "            df = pd.DataFrame(self.rows)\n",
    "            self.dfs.append(df)\n",
    "            self.rows = []\n",
    "        \n",
    "        # Process Input\n",
    "        df_input = pd.concat(self.dfs)\n",
    "        df_input['TIMESTAMP'] = pd.to_datetime(df_input['TIMESTAMP'])\n",
    "        df_input.groupby('TIMESTAMP').sum('VALUE').reset_index()\n",
    "        df_input = df_input[['TIMESTAMP','VALUE']]\n",
    "\n",
    "        #Train + Forecast Length\n",
    "        train_length = 600\n",
    "        forecast_horizon = 30\n",
    "        train_end = max(df_input['TIMESTAMP'])\n",
    "        train_start = train_end - pd.Timedelta(days = 600)\n",
    "        \n",
    "        df_input = df_input.loc[(df_input['TIMESTAMP'] >= train_start) &\n",
    "                                (df_input['TIMESTAMP'] < train_end)].reset_index(drop=True)\n",
    "        df_input = df_input.set_index('TIMESTAMP')\n",
    "        df_input.index.name = 'time'\n",
    "        \n",
    "        # Convert DataFrame to Darts TS Object\n",
    "        ts_train = TimeSeries.from_dataframe(df_input, fill_missing_dates=True, freq='D')\n",
    "        \n",
    "        # Initialize Model\n",
    "        FFT_model = FFT(nr_freqs_to_keep=400,trend=None)\n",
    "        \n",
    "        # Fit Model and Predict\n",
    "        FFT_model.fit(ts_train)\n",
    "        ts_forecast = FFT_model.predict(forecast_horizon)\n",
    "        \n",
    "        # Processing\n",
    "        data = ts_forecast.pd_dataframe().reset_index().values\n",
    "        df_forecast = pd.DataFrame(data, columns = ['TIMESTAMP','VALUE'])\n",
    "        df_forecast['TRAIN_START'] = train_start\n",
    "        df_forecast['TRAIN_END'] = train_end\n",
    "        df_forecast['FORECAST_HORIZON'] = forecast_horizon\n",
    "\n",
    "        yield from df_forecast.itertuples(index=False, name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff6c817",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = session.table('TIME_SERIES_DATA') \\\n",
    "        .with_column('ROW', F.object_construct_keep_null('*')) \\\n",
    "        .select(F.col('PUMP_ID'), F.col('ROW'))\n",
    "\n",
    "store_forecast_test = F.table_function(\"PUMP_TS_DARTS_FFT\")\n",
    "\n",
    "variant_column = F.parse_json(df.col('ROW').cast(T.VariantType()))\n",
    "\n",
    "forecast = df.select(\n",
    "                F.col('PUMP_ID'), \n",
    "                store_forecast_test(variant_column).over(partition_by=['PUMP_ID'])\n",
    "                )\n",
    "\n",
    "forecast = forecast.with_column('MODEL', F.lit('FFT'))\n",
    "forecast = forecast.with_column('FORECAST_DATETIME', F.current_timestamp())\n",
    "forecast.write.save_as_table(\"FORECAST_USING_DARTS_FFT\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f508c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = session.table('FORECAST_USING_DARTS_FFT')\n",
    "df_temp.limit(5).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e7801c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54ce9d1e",
   "metadata": {},
   "source": [
    "## - Darts with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0231f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = T.StructType([\n",
    "    T.StructField(\"TIMESTAMP\", T.DateType()),\n",
    "    T.StructField(\"FORECAST\", T.IntegerType()),\n",
    "    T.StructField(\"TRAIN_START\", T.DateType()),\n",
    "    T.StructField(\"TRAIN_END\", T.DateType()),\n",
    "    T.StructField(\"FORECAST_HORIZON\", T.IntegerType())\n",
    "])\n",
    "\n",
    "@F.udtf(output_schema = schema,\n",
    "        input_types = [T.VariantType()],\n",
    "        name = \"PUMP_TS_DARTS_XGB\", is_permanent=True, stage_location=\"@ML_MODELS\", session=session,\n",
    "        packages=['pandas', 'fsspec==2023.4.0','holidays==0.18',\n",
    "                  'joblib==1.2.0','lightning-utilities==0.7.1','matplotlib==3.7.1',\n",
    "                  'plotly==5.9.0','pmdarima==2.0.3','pytorch==2.0.1',\n",
    "                  'pytorch-lightning==2.0.3','pyyaml==6.0','scikit-learn==1.2.2',\n",
    "                  'scipy==1.10.1','snowflake-snowpark-python==1.4.0','statsmodels',\n",
    "                  'tbats==1.1.3','torchmetrics==0.11.4','tqdm','xarray','xgboost'\n",
    "                 ], replace=True,\n",
    "        imports = [\"@ML_MODELS/nfoursid.zip\", \"@ML_MODELS/darts.zip\"])\n",
    "\n",
    "class forecast:\n",
    "    def __init__(self):\n",
    "        self.rows=[]\n",
    "        self.dfs=[]\n",
    "    \n",
    "    def process(self, data):\n",
    "        self.rows.append(data)\n",
    "\n",
    "        # Merge rows into a dataframe\n",
    "        if len(self.rows) >= 16000:\n",
    "            df = pd.DataFrame(self.rows)\n",
    "            self.dfs.append(df)\n",
    "            self.rows = []\n",
    "        \n",
    "        # Merge dataframes into a single dataframe\n",
    "        # Minimizes memory footprint\n",
    "        if len(self.dfs) >= 100:\n",
    "            merged_df = pd.concat(self.dfs)\n",
    "            self.dfs = [merged_df]\n",
    "\n",
    "        yield None\n",
    "    \n",
    "    def end_partition(self):\n",
    "        # Merge any remaining rows\n",
    "        from darts import TimeSeries\n",
    "        import xgboost\n",
    "        from darts.models import XGBModel\n",
    "        \n",
    "\n",
    "        if len(self.rows) > 0:\n",
    "            df = pd.DataFrame(self.rows)\n",
    "            self.dfs.append(df)\n",
    "            self.rows = []\n",
    "        \n",
    "        # Process Input\n",
    "        df_input = pd.concat(self.dfs)\n",
    "        df_input['TIMESTAMP'] = pd.to_datetime(df_input['TIMESTAMP'])\n",
    "        df_input.groupby('TIMESTAMP').sum('VALUE').reset_index()\n",
    "        df_input = df_input[['TIMESTAMP','VALUE']]\n",
    "\n",
    "        #Train + Forecast Length\n",
    "        train_length = 600\n",
    "        forecast_horizon = 30\n",
    "        train_end = max(df_input['TIMESTAMP'])\n",
    "        train_start = train_end - pd.Timedelta(days = 600)\n",
    "        \n",
    "        df_input = df_input.loc[(df_input['TIMESTAMP'] >= train_start) &\n",
    "                                (df_input['TIMESTAMP'] < train_end)].reset_index(drop=True)\n",
    "        df_input = df_input.set_index('TIMESTAMP')\n",
    "        df_input.index.name = 'time'\n",
    "        \n",
    "        # Convert DataFrame to Darts TS Object\n",
    "        ts_train = TimeSeries.from_dataframe(df_input, fill_missing_dates=True, freq='D')\n",
    "        \n",
    "        # Initialize Model\n",
    "        my_model = XGBModel(lags = 10, n_estimators=100, max_depth=5)\n",
    "\n",
    "        # Fit Model and Predict\n",
    "        my_model.fit(ts_train)\n",
    "        ts_forecast = my_model.predict(forecast_horizon)\n",
    "        \n",
    "        # Processing\n",
    "        data = ts_forecast.pd_dataframe().reset_index().values\n",
    "        df_forecast = pd.DataFrame(data, columns = ['TIMESTAMP','VALUE'])\n",
    "        df_forecast['TRAIN_START'] = train_start\n",
    "        df_forecast['TRAIN_END'] = train_end\n",
    "        df_forecast['FORECAST_HORIZON'] = forecast_horizon\n",
    "\n",
    "        yield from df_forecast.itertuples(index=False, name=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334ce005",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = session.table('TIME_SERIES_DATA') \\\n",
    "        .with_column('ROW', F.object_construct_keep_null('*')) \\\n",
    "        .select(F.col('PUMP_ID'), F.col('ROW'))\n",
    "\n",
    "store_forecast_test = F.table_function(\"PUMP_TS_DARTS_XGB\")\n",
    "\n",
    "variant_column = F.parse_json(df.col('ROW').cast(T.VariantType()))\n",
    "\n",
    "forecast = df.select(\n",
    "                F.col('PUMP_ID'), \n",
    "                store_forecast_test(variant_column).over(partition_by=['PUMP_ID'])\n",
    "                )\n",
    "\n",
    "forecast = forecast.with_column('MODEL', F.lit('XGB'))\n",
    "forecast = forecast.with_column('FORECAST_DATETIME', F.current_timestamp())\n",
    "forecast.write.save_as_table(\"FORECAST_USING_DARTS_XGB\", mode=\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cd5b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_temp = session.table('FORECAST_USING_DARTS_XGB')\n",
    "# df_temp.limit(5).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2ebc4e-ba93-4d36-a4e9-f9ba80a461dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63369f08-f95d-4139-8c6d-1a250b381b40",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "Two options -\n",
    "1. Create a task using SQL\n",
    "2. Create a task using Task API (future improvement)\n",
    "\n",
    "The code given below uses option 1 to create a task in SQL.\n",
    "1. Get the command to run the forecast function\n",
    "2. Create a task\n",
    "3. Resume task to run on the schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e904399f-367f-4778-bf0b-1c35e0c905f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_script = f'''\n",
    "INSERT  INTO FORECAST_USING_DARTS_XGB  SELECT \"PUMP_ID\", \"TIMESTAMP\", \"FORECAST\", \"TRAIN_START\", \"TRAIN_END\",\n",
    "\"FORECAST_HORIZON\", 'XGB' AS \"MODEL\", current_timestamp() AS \"FORECAST_DATETIME\"\n",
    "FROM ( SELECT T_LEFT.*, T_RIGHT.\"TIMESTAMP\", T_RIGHT.\"FORECAST\", T_RIGHT.\"TRAIN_START\", T_RIGHT.\"TRAIN_END\", T_RIGHT.\"FORECAST_HORIZON\"\n",
    "FROM ( SELECT \"PUMP_ID\", object_construct_keep_null(*) AS \"ROW\" FROM TIME_SERIES_DATA) AS T_LEFT\n",
    "JOIN  TABLE (PUMP_TS_DARTS_XGB(parse_json( CAST (\"ROW\" AS VARIANT)))  OVER (PARTITION BY \"PUMP_ID\" )) AS T_RIGHT)\n",
    "'''\n",
    "\n",
    "task_script = f'''\n",
    "CREATE OR REPLACE TASK POC_INVISTA.TASK_FORECAST_TS\n",
    "WAREHOUSE = SSK_RESEARCH\n",
    "SCHEDULE = '1 MINUTE'\n",
    "AS {forecast_script};\n",
    "'''\n",
    "\n",
    "resume_script = f'''ALTER TASK POC_INVISTA.TASK_FORECAST_TS RESUME;'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a458bd5d-661f-4837-a286-ca6025794854",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(task_script).collect()\n",
    "# session.sql(resume_script).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7031f764-d987-4324-aedf-5a6bfe4fe2e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d8d1d6-6db4-4f63-a252-63d73157f356",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
